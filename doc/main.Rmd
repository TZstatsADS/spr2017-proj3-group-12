---
title: "Project 3 - Example Main Script"
author: "Yuting Ma, Tian Zheng"
date: "February 24, 2016"
output:
  pdf_document: default
  html_document: default
---
Revlevant packages needed for this file

```{r}
list.of.packages <- c("e1071", "ggplot2","gbm","caret","randomForest","EBImage")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) 
  {
   install.packages(new.packages)
   source("https://bioconductor.org/biocLite.R")
   biocLite("EBImage")
  }

library("gbm")
library("ggplot2")
library("caret")
library("randomForest")
library("EBImage")
```

### Step 0: specify directories.

```{r wkdir, eval=FALSE}
#setwd("./ads_spr2017_proj3/lib")
setwd("../spr2017-proj3-group-12/lib")
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
```

Provide directories for raw images. Training set and test set should be in different subfolders. 
```{r}
#image_test_dir <- "../data/test_data/raw_images" # This will be modified for different data sets.
#image_train_dir <- "../data/train_data/raw_images"
#img_train_dir <- paste(experiment_dir, "train/", sep="")
#img_test_dir <- paste(experiment_dir, "test/", sep="")
image_all.dir <- "../data/training_data/raw_images"
original_data_train = "../data/sift_ori_train.csv"
original_data_test = "../data/sift_ori_test.csv"
modified_data_train = "../data/sift_simp_gray_train.csv"
modified_data_test = "../data/sift_simp_gray_test.csv"
labels_train = "../data/labels_train.csv"
labels_test = "../data/labels_test.csv"

gbm_model_original_features = "../output/GBMFullFeature.RData"
rf_model_original_features = "../output/RFFullFeature.RData"
gbm_model_modified_features = "../output/GBMModifiedFeature.RData"
rf_model_modified_features = "../output/RFModifiedFeature.RData"

gbm_model_original_predict = "../output/GBMFullFeaturePredictions.csv"
rf_model_original_predict = "../output/RFFullFeature.RData"
gbm_model_modified_predict = "../output/GBMModifiedPredictions.csv"
rf_model_modified_predict = "../output/RFModifiedFeature.RData"
```

### Step 1: set up controls for evaluation experiments.

In this chunk, ,we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set for GBM
+ (number) K, the number of CV folds
+ (T/F) Out of Bag Estimate (similar to cross-validation) on training set for Random Forest
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set


```{r exp_setup}
run.cv=FALSE # run cross-validation on the training set
K <- 5  # number of CV folds
run.OOB=FALSE
run.feature.train=TRUE # process features for all pictures
run.test=TRUE # run evaluation on an independent test set
#run.feature.test=TRUE # process features for test set
```

Using cross-validation or independent test set evaluation, we compare the performance of different classifiers or classifiers with different specifications. In this example, we use GBM with different `depth`. In the following chunk, we list, in a vector, setups (in this case, `depth`) corresponding to models that we will compare. In your project, you maybe comparing very different classifiers. You can assign them numerical IDs and labels specific to your project. 

```{r model_setup, eval=FALSE, include=FALSE}
#model_values <- seq(3, 11, 2)
#model_labels = paste("GBM with depth =", model_values)
```

### Step 2: import training images class labels.

Class "0" for fried chicken and Class "1" for dogs

```{r train_label}
label_train <- c(rep(0,1000),rep(1,1000))
```

### Step 3: construct visual feature for Full images

`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features that are required by all the models you are going to evaluate later. 

```{r feature}
source("../lib/feature.R")
source("../lib/DataSplit.R")

tm_feature <- NA
if(run.feature.train){
  #tm_feature <- system.time({
   # dat_all <- feature(img_dir=image_all.dir)})
  #tm_feature <- 
    feature(img_dir=image_all.dir) 
                                     #feature(img_train_dir, 
                                     #                  "train", 
                                      #                 data_name="zip", 
                                       #                export=TRUE))
}

#SPlit the data in to train and test sets

dataSplit.cv()

#tm_feature_train <- NA
#if(run.feature.train){
 # tm_feature_train <- system.time({
  #  dat_train <- feature(img_dir=image_train_dir)})
                                    #feature(img_train_dir, 
                                     #                  "train", 
                                      #                 data_name="zip", 
                                       #                export=TRUE))
#}




#tm_feature_test <- NA
#if(run.feature.test){
 # tm_feature_test <- system.time(dat_test <- feature(img_test_dir, 
  #                                                   "test", 
   #                                                  data_name="zip", 
    #                                                 export=TRUE))
#}

#write(dat_all,file="../output/feature_all.csv")
save(dat_all, file="../output/feature_all.RData")
#save(dat_train, file="./output/feature_train.RData")

#save(dat_test, file="./output/feature_test.RData")
```

### Step 4: Train a classification model with training images
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
  + Input: a path that points to the training set features.
  + Input: an R object of training sample labels.
  + Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
+ `test.R`
  + Input: a path that points to the test set features.
  + Input: an R object that contains a trained classifiers.
  + Output: an R object of class label predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib}
source("../lib/train.R")
source("../lib/test.R")
source("../lib/GBM.R")
source("../lib/RandomForest.R")
#Constructing training and testing files(Stored under DataSplit.R)
#dataSplit.cv(percentage = 0.25, dat.all)
```

#### Model Training for GBM and Random Forest (with optional CV/OOB parameter selection)

Training models on both the original features and the modified fatures
```{r runcv, message=FALSE, warning=FALSE}
#source("../lib/cross_validation.R")

train(original_data_train, labels_train, full_feature = TRUE, run_cv = run.cv, run_OOB = run.OOB, K = K)
train(modified_data_train, labels_train, full_feature = TRUE, run_cv = run.cv, run_OOB = run.OOB, K = K)

```

####GBM Cross Validation Results
Visualize cross-validation results for GBM. 
```{r cv_vis}
if(run.cv){
  ##### INSERT PHOTOS HERE
  

  #pdf("../fig/cv_results.pdf", width=7, height=5)
  #plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
   #    main="Cross Validation Error", type="n", ylim=c(0, 0.25))
  #points(model_values, err_cv[,1], col="blue", pch=16)
  #lines(model_values, err_cv[,1], col="blue")
  #arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2], 
   #     length=0.1, angle=90, code=3)
  #dev.off()
}
```

* What is the best choice of parameters?
```{r best_model}
#model_best=model_values[1]
if(run.cv){
  #model_best <- model_values[which.min(err_cv[,1])]
}

#par_best <- list(depth=model_best)
```


####Random Forest OOB Results

Visualize OOB error
```{r cv_vis}
if(run.OOB){
  #PHOTOS HERE
  
  
  #pdf("../fig/cv_results.pdf", width=7, height=5)
  #plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
   #    main="Cross Validation Error", type="n", ylim=c(0, 0.25))
  #points(model_values, err_cv[,1], col="blue", pch=16)
  #lines(model_values, err_cv[,1], col="blue")
  #arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2], 
    #    length=0.1, angle=90, code=3)
  #dev.off()
}
```

* Choose the best number of trees
```{r best_model}
model_best=model_values[1]
if(run.OOB){
  #model_best <- model_values[which.min(err_cv[,1])]
}

#par_best <- list(depth=model_best)
```


### Step 5: Make predictions on test data

#For original features
```{r test}
tm_test=NA
if(run.test){

  load(gbm_model_original_features)
  load(rf_model_original_features)
  
  #load(file=paste0("../output/feature_", "zip", "_", "test", ".RData"))
  #load(file="../output/fit_train.RData")
  #tm_test <- system.time(pred_test <- test(fit_train, dat_test))
  #save(pred_test, file="../output/pred_test.RData")
}
```

#For test feature
```{r test}
tm_test=NA
if(run.test){
  load(gbm_model_modified_features)
  load(rf_model_modified_features)
  test(tune_gbm, image_rf, "../data/sift_simp_gray_test.csv", full_feature = FALSE)
  rf_predict = read.csv("../output/RFModifiedPredictions.csv")
  gbm_predict = read.csv("../output/GBMModifiedPredictions.csv")
  test_labels = unlist(read.csv("../data/labels_test/csv"))
  rf_error = sum(rf_predict != test_labels)/length(test_labels)
  gbm_error = sum(gbm_predict != test_labels)/length(test_labels)
  cat("GBM error for modified features is ", gbm_error, "/n")
  cat("Random Forest error for modified features is, ", rf_error, "/n")
  
  #load(file=paste0("../output/feature_", "zip", "_", "test", ".RData"))
  #load(file="../output/fit_train.RData")
  #tm_test <- system.time(pred_test <- test(fit_train, dat_test))
  #save(pred_test, file="../output/pred_test.RData")
}
```

### Summarize Performance of various models
Prediction performance matters, do does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for constructing all features=", tm_feature[1], "s \n")
#cat("Time for constructing training features=", tm_feature_train[1], "s \n")
#cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for making prediction=", tm_test[1], "s \n")
```
